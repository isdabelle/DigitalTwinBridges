{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Synthesizer\n",
    "\n",
    "For the BN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('CeneriData/cleaned_2003_2019_platoon.pkl')\n",
    "\n",
    "ax_cols = []\n",
    "for i in range(1, 10):\n",
    "    ax_cols.append('{}_{}'.format('AX_W', i))\n",
    "    #for j in range(0, 10):\n",
    "        #ax_cols.append('{}_{}_{}'.format('AX_W', i, j))\n",
    "\n",
    "for i in range(1, 10):\n",
    "    ax_cols.append('{}_{}'.format('AX_DIST', i))\n",
    "    #for j in range(0, 9):\n",
    "        #ax_cols.append('{}_{}_{}'.format('AX_DIST', i, j))\n",
    "\n",
    "df_noax = df.drop(columns = ax_cols)\n",
    "\n",
    "no_col = []\n",
    "for col in df_noax.columns:\n",
    "    if col[-1].isdigit() and int(col[-1]) > 5:\n",
    "        no_col.append(col)\n",
    "\n",
    "df_sm = df_noax.drop(columns=no_col)\n",
    "\n",
    "import random\n",
    "\n",
    "df_sm = df_sm.astype(int)\n",
    "\n",
    "df_sm.columns\n",
    "\n",
    "#Comparing platoon lengths of 5\n",
    "df_5 = df_sm[df_sm.Length == 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save df_5 to csv so it can be accessed by the data synthesizer\n",
    "df_5.to_csv('df_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSynthesizer.DataDescriber import DataDescriber\n",
    "from DataSynthesizer.DataGenerator import DataGenerator\n",
    "from DataSynthesizer.ModelInspector import ModelInspector\n",
    "from DataSynthesizer.lib.utils import read_json_file, display_bayesian_network\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dataset\n",
    "input_data = 'df_5.csv'\n",
    "# location of two output files\n",
    "mode = 'correlated_attribute_mode'\n",
    "description_file = f'description.json'\n",
    "synthetic_data = f'sythetic_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attribute is categorical if its domain size is less than this threshold.\n",
    "# Here modify the threshold to adapt to the domain size of \"education\" (which is 14 in input dataset).\n",
    "threshold_value = 20\n",
    "\n",
    "# specify categorical attributes\n",
    "categorical_attributes = {'CLASS_1': True, 'CLASS_2':True,'CLASS_3':True,'CLASS_4':True,'CLASS_5':True, 'Length':True,\n",
    "                    'Weekday':True,'Hour':True,'AX_1':True,'AX_2':True,'AX_3':True,'AX_4':True,'AX_5':True}\n",
    "\n",
    "# specify which attributes are candidate keys of input dataset.\n",
    "#candidate_keys = {'ssn': True}\n",
    "\n",
    "# A parameter in Differential Privacy. It roughly means that removing a row in the input dataset will not \n",
    "# change the probability of getting the same output more than a multiplicative difference of exp(epsilon).\n",
    "# Increase epsilon value to reduce the injected noises. Set epsilon=0 to turn off differential privacy.\n",
    "epsilon = 0\n",
    "\n",
    "# The maximum number of parents in Bayesian network, i.e., the maximum number of incoming edges.\n",
    "degree_of_bayesian_network = 20\n",
    "\n",
    "# Number of tuples generated in synthetic dataset.\n",
    "num_tuples_to_generate = 10000 # Here 32561 is the same as input dataset, but it can be set to another number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Constructing Bayesian Network (BN) ================\n",
      "Adding ROOT AX_2\n",
      "Adding attribute CLASS_2\n",
      "Adding attribute LENTH_2\n",
      "Adding attribute GW_TOT_2\n",
      "Adding attribute Hour\n",
      "Adding attribute LENTH_1\n",
      "Adding attribute CLASS_1\n",
      "Adding attribute GW_TOT_1\n",
      "Adding attribute LENTH_4\n",
      "Adding attribute LENTH_5\n",
      "Adding attribute GW_TOT_3\n",
      "Adding attribute GW_TOT_5\n",
      "Adding attribute GW_TOT_4\n",
      "Adding attribute LENTH_3\n",
      "Adding attribute CLASS_5\n",
      "Adding attribute CLASS_3\n",
      "Adding attribute CLASS_4\n",
      "Adding attribute Weekday\n",
      "Adding attribute SPEED_5\n",
      "Adding attribute AX_1\n",
      "Adding attribute SPEED_3\n",
      "Adding attribute AX_5\n",
      "Adding attribute AX_3\n",
      "Adding attribute AX_4\n"
     ]
    }
   ],
   "source": [
    "describer = DataDescriber(category_threshold=threshold_value)\n",
    "describer.describe_dataset_in_correlated_attribute_mode(dataset_file=input_data, \n",
    "                                                        epsilon=epsilon, \n",
    "                                                        k=degree_of_bayesian_network,\n",
    "                                                        attribute_to_is_categorical=categorical_attributes)\n",
    "describer.save_dataset_description_to_file(description_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator()\n",
    "generator.generate_dataset_in_correlated_attribute_mode(num_tuples_to_generate, description_file)\n",
    "generator.save_synthetic_data(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(input_data, skipinitialspace=True)\n",
    "synthetic_df = pd.read_csv(synthetic_data)\n",
    "# Read attribute description from the dataset description file.\n",
    "attribute_description = read_json_file(description_file)['attribute_description']\n",
    "\n",
    "inspector = ModelInspector(input_df, synthetic_df, attribute_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for attribute in synthetic_df.columns:\n",
    "    inspector.compare_histograms(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector.mutual_information_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "traffic"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
